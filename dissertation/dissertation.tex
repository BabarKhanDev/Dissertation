\documentclass{UoYCSproject}
% Preamble
\usepackage{url}     % This is needed for IEEE referencing
\usepackage{bookmark}
\usepackage{algpseudocode} % Pseudocode algorithm
\usepackage{algorithm}
\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}

% Title Page
\author{Babar Khan}
\title{Predicting Memorable Regions Of Images Using Deep Learning and Adversarial Networks}
\date{Version 1, 2023-April-11}
\supervisor{Adrian Bors}
\SWE

% Document
\begin{document}
\pagenumbering{roman}
\maketitle

\newpage{}

\tableofcontents

\newpage{}

\chapter{Introduction}

% This should be 2 pages
% need to define well what i am doing

\section{Background}

\subsection{We see a lot of images}

We are constantly surrounded by imagery, on the way to work, on the internet, on TV, in stores, etc. Some of them stick out more than others, we see hundreds, if not thousands, of images a day, and yet culturally and individually we all remember the same ones.

\subsection{Cultural vs Individual Memorability}

Due to cultural significance an image can become memorable. The 2015 dress \cite{BBCDress2015}, a country's flag, or the 1932 image “lunch atop a skyscraper” \cite{gambino_2012}, come to mind. I would argue that the memorability of these images is tied to the culture surrounding them, not necessarily due to intrinsic properties within. The focus of this research is on the intrinsic properties within an image that make it memorable to an individual, like seeing an advert on a bus and then later recognising the same advert online.

\subsection{How do we measure the memorability of an image}

% What work has shown independence of memorability from viewer?
% Isola et al in 2011
% Vischema dataset experiments

It has been shown that the memorability of an image is an intrinsic property, independent from the viewer \cite{Isola2011, IsolaParikhTorralbaOliva2011, ICCV15_Khosla, isola2014memorability}. This was achieved by performing a memorability game where participants are presented with a stream of images, and on an interval shown an image that they had already seen. Most participants were able, or unable, to remember the same images.
%I need to check if this is accurate for each of those sources ^^^^%
How often an image was recognised is proportional to its memorability. %Verify%
This is taken further by Akagunduz et al. \cite{VischemaPaper} where, in a similar experiment, they measure which regions of images are memorable.

\subsection{Why do we want to create a memorable image}

\subsection{How does this research help to create a memorable image}

\newpage{}

\section{Motivation}

% Why do we want to do this
% Who will this be useful for
% What motivates me

\section{What are my goals}

% To create an accurate mapping from images to memorable images
% How do I define accuracy?
%   This is dataset specific but as I am using the vischema dataset I can measure accuracy as the L1 Loss between the produced memorability map and the ones present in the dataset.

\section{Structure}

% How is the dissertation laid out.

\newpage{}

\chapter{Literature Review}

% This should be 6-8 pages long

\section{Memorability}

% some much earlier work on the study of memorability

% Roy Brener An experimental investigation of memory span
% Subjects were presented with units of varying length and were asked to recall them
% The unit could be a sentence, a digit, a geometric shape, etc
% It was found that people are not very good at remembering nonsense sylables or sentences but are much better at remembering digits, consonants, and colours. Remembering geometric designs is placed somewhere in the middle.
% This would seem to indicate that people are not very good at recalling images, however in Nickerson's work we can see that subjects are extremely good at recognising images. 

% R. Nickerson Short-term memory for complex meaningful visual configurations 1965
% Subjects are shown new and old photos and they have to identify if the photo is new or old
% P(Ro/So) = ~0.9
% 95% of all responses were correct
% subjects were more likely to correctly identify a photo as new than they were to correctly identify a photo as old
% Shown 1 image at a time with 50% chance of being old and 50% chance of being new
% 

% L. Standing. Learning 10,000 pictures. In Quarterly Journal of Experimental Psychology, 1973
% ## What is the goal of this paper ##
% Memory capacity and retrieval of pictures and words
% it shows that the capacity for memory of pictures is almost limitless, when measured under appropriate conditions. 
% People are able to recall images incredibly well  
% ## What are its limitations ##
% The experiments don't have many people involved Experment 2 only has 2 subjects and experiment 4, 4.
% He tested normal and vivid pictures
%------------

% F. Brady, T. Konkle, G. A. Alvarez, and A. Oliva. Visual long-term memory has a massive storage capacity for object details. In Proceedings of the National Academy of Sciences, 2008
% This is about how much detail is required to remember an image, rather than if you can just remember it or not
% Are you truly only able to store the gist of items in your long term memory, or are we capable of more?
% It is shown that people are able to store far more information than just the gist in their long term memory, we are able to store fine grained details
% this builds directly onto the world of Standing in 1973
%------------

In R. Breners work [An experimental investigation of memory span] subject's abilities to remember a series of units was tested, a unit was different depending on the test, 
\begin{quote}
    In the digit test, for example, each digit was a unit; in the sentence test each sentence was a unit, etc. ... Each nonsense syllable constituted a unit ... Each consonant constituted a unit ... Each [geometric] design constituded a unit. 
\end{quote}
It was found that people are not very good at remembering nonsense sylables or sentences but are much better at remembering digits, consonants, and colours. Remembering geometric designs is placed somewhere in the middle. This is of interest because it's very similar to our investigation into what aspects make images memorable. Brener was interested in consonants, colours, geometric designs, etc but the finding that different units can be significantly harder or easier to remember is useful to us. If we swap out units for image properties such as texture, contrast, saturation etc, or even more abstract features that a CNN may recognise, then it should be possible to find how memorabilty is impacted by these.   

The mean score of 5.31 found for geometric designs seems to indicate that people are not great at recalling images, however in R. Nickerson's work [Short-term memory for complex meaningful visual configurations] we can see the opposite, subjects are found to be extremely good at recognising images. Nickerson found that subjects shown a series of images are, with great accuracy, able to recall if the photo is one they have seen before or not. An item is referred to as "new on its first occurrence and old on its second occurrence"[Short-term memory for complex meaningful visual configurations p.156], it was found that subjects shown one image at a time, each with equal chance of being old or new, are able to correctly distinguish them with 95\% accuracy. 

Like in [Short-term memory for complex meaningful visual configurations],  R. Shephards work [Recognition Memory for Words, Sentences, and Pictures] also found that subjects are able to distinguish with great accuracy new and old images, the mean percentage of images correctly identified was 99.7\% after a delay 2 hours, 92.0\% after a delay of 3 days. 87.0\% after a delay of 7 days, and 57.7\% after a delay of 120 days. The introduction of a delay into Nickerson's experiment allows us to see that regardless of whether the image is in our short-term memory (2 hour delay) or our long-term memory (3-120 days) subjects are able to correctly identify an incredible amount of images. 

L. Standing built on the work of Nickerson, Shepard, and Brener in [Learning 10,000 pictures]. Four experiments were ran which tested memory capacity and retreival speed for pictures and words, I am interested in the performance related to pictures specifically. He found that the capacity for image recognition from memory is almost limitless, when measured under testing conditions. In Standing's first experiment he tested 'normal' and 'vivid' images. He describes 'normal' images as those that "may be characterized as resembling a highly variegated collection of competent snapshots"[Learning 10,000 pictures, p.208], and 'vivid' images are described as "striking pictures ... with definitely interesting subject matter"[Learning 10,000 pictures, p.208]. Much like how in Brener's work [An experimental investigation of memory span] the memorability of an object was variable based on what classification of unit was tested, this work shows that on an even more specific level, the classification of the image into normal or vivid, has an impact on subjects memorabilty, Standing found that the vivid images were more likely to be remembered by subjects.

Lots of work has been done to further build on that of Standing, Nickerson, Brener et al. T. Brady et al. [Visual long-term memory has a massive storage capacity for object details] performed research into how much information is retained in long term memory and found that subjects are able to remember not just the gist of an image, but also fine grain information such as the state of objects within an image, and that they are able to distinguish between variants of objects shown in images. An example shown is an abacus in two different states, 13/14 subjects were able to distinguish which one they had seen before.

% D. A. Brown, I. Neath, and N. Chater. A temporal ratio model of memory. Psych. Review, 2007.
%An exploration to the extend to which common retrieval princples apply to human memory over short and long timescales
% Traces of items are represented in memory partyly in terms of how long ago they were observed
% Similar mechanisms govern retrieval from memory over many different timescales

T. Konkle et al. build further on the work in [Learning 10,000 pictures] and [Visual long-term memory has a massive storage capacity for object details] by studying the impact that categorical distinctness has on memorability. This was tested by creating a dataset of images where composed of categories such as tables, cameras, and bread etc. Each category had between 1 and 16 images, a memory test like those performed in [Learning 10,000 pictures] and [Visual long-term memory has a massive storage capacity for object details] etc was performed and the percentage correctly identified was found to decrease as the number of images within a category increased. From this we can see that categorically distinct images are more likely to be remembered. 

Studies by Isola et al. [1, 11] were performed with the goal of identifying a collection of visual attributes that make images memorable, and to use those to predict the memorability of an image. It was found that properties such as mean saturation, and the log number of objects, has less impact on the memorability score than object statistics. 
%WTF are object statistics?
Categories such as: person, person sitting, and floor, were most helpful image memorability. The categories: ceilings, buildings, and mountains, were least helpful. Their approach was limited by the fact that the object statistics were annotated by hand, this would both make automating the process of determining image memorability impossible, and limit them from finding any abstract properties that helped/hindered memorability.

Khosla et al. [4] built on the work in [1, 11] by, instead of determining memorability of an entire image, creating a model that discovers memorability maps of individual images without human annotation. These memorability maps are able to distinguish which areas of an image are remembered, forgotten, or hallucinated. Their approach, similar to Isola et al. in [1, 11], is limited by the arbitrarily picked list of features that define memorability.

\section{Using Deep Learning to Predict Memorability}

% How do I want this to flow?
%   Deep learning intro? talk about it being an image segmentation problem and something about convolution and learning filters
%   Possible Datasets
%   Auto Encoders (UNET)
%   GANS
%   Diffusion
%   Alternatives (resnets etc.)

\subsection{Deep learning has seen great success with image synthesis}
% THIS IS MOSTLY CRAP AND NEEDS MAJOR REWORKING AND FOCUSING
Recently, the use of deep convolutional networks [12,13] has proven exceptionally successful in image classification and object recognition. It stands to reason that a combination of these systems could perform well at this task. It could, in the worst case, recognise which objects are within an image, and to use that to help inform how memorable the image is. In the best case, it could learn to extract the features from categories that make them memorable and use this to predict the memorability of categories not yet seen. The weights within a convolutional layer act as filters to recognise patterns within images, through the stacking of these filters we are able to build networks that can recognise complex objects. The main limitation within [1,11,4] is that the features are arbitrarily picked by humans and manually annotated. The use of convolution should make it possible to learn these features automatically.   

% I want to work on this paragraph heavily, maybe even split it into two and discuss more in extensive detail.
Khosla et al. have created a dataset, LaMem [5], containing 60,000 annotated image and memorability pairs. They use deep learning to predict these memorability maps, this is similar to the VISCHEMA plus dataset [7], which I will be working with. In [7] the network output utilises fully connected layers which I believe is unnecessary, and may even hurt performance. More modern computer vision network structures, such as the U-Net [8] or ResNet [9], use fully convolutional networks. These maintain spatial locality which makes them less prone to overfitting, and also allows them to generalise better. Using deep learning we should be able to automate the process of learning features like those in [1,4,11], by training a fully convolutional network with a sufficiently large dataset such as those presented in [5,7].

% image segmentation?
I propose that predicting memorability maps is an extension of image segmentation, modern machine learning has found many network architectures that would be worth exploring for this problem

\subsection{Auto-Encoders}
% in 2015 the UNet was introduced, it is an auto encoder that has seen great success

% introduce auto encoders, their weaknesses and their benefits
    % https://arxiv.org/abs/2003.05991 - Auto-Encoders
    % Talk about them mathematically, talk about their success in compression and decompression
    % https://medium.com/edureka/autoencoders-tutorial-cfdcebdefe37 - Auto encoders
    % http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf - Original Autoencoder paper 
% then talk about the UNet and how historically significant it is and why
    % it is an auto encoder with skip connections - these allow the later decoding layers to retain some of the initial information

The U-Net [8] has also found great success in image segmentation, this system however relies on the output being a classification. Something in between these two could work very well.

I will use a U-Net [U-Net paper] as my image to image network. The U-Net makes use of an Encoder block and a Decoder block, there are residual connections between different sections of the blocks that help to preserve image data in the decoding stage. The network has seen great success in the medical field. [U-net success paper].

\subsection{Generative Adversarial Networks}
% in 2018? ns on the date, pix2pix was invented, it is a GAN built around the UNet and has also seen great success

% need to introduce GANs, their weaknesses and their benefits

% You typically cant track the progress of a GAN by looking at the loss of the generator or the discriminator, however after running an epoch of training I can compute the loss of the generator across the validation dataset, independent of the discriminator, this allows us to track the progress of the generator and compare it to other methods.


% Maybe here say what a GAN vs a conditional GAN is 

Conditional GAN architectures have seen lots of success and produce high quality images, they work through the use of competitive co-evolutionary algorithms where a Generator and a Discriminator will compete. The Generator is typically given a latent space vector and uses that to produce an image, the Discriminator has to determine if that image belongs to a given distribution. In conditional GANS The weights of the two networks are updated through backpropagation based on if they win or lose. The Generator wins by fooling the Discriminator into predicting that the image is from the distribution. The Discriminator wins if it can accurately predict whether an image is from the distribution or not. These networks have seen great success but typically suffer from instability due to their adversarial nature, and often suffer from mode collapse. Despite this, lots of research has gone into producing GANs that produce high quality work.  

Pix2Pix by Isola et al. [10], is an image-to-image deep learning architecture based on Conditional Generative Adversarial Networks. It is able to convert images from one style into another. For example, it can convert labels into a street scene or photos of daytime into night time. It makes use of a UNet backbone and has found great success. 
\subsection{Diffusion Models}

% talk about significance of diffusion, its benefits and weaknesses
% how it builds ontop of GANs and why they have seen so much success recently

Diffusion based image generation is inspired by nonequilibrium thermodynamics, it works through a forward diffusion process of adding Gaussian noise to a sample until it has been transformed such that it is no longer distinguishable to Gaussian noise, this should follow a diffusion schedule where noise is applied T times. We then train a model to compute a reverse diffusion process. That is, given a value 1 < t < T, and a sample that has been transformed t times, predict the sample at t-1 transformations. When we want to produce a sample, we create $ x_0 \sim N(0,1) $ and apply reverse diffusion to it T times, following our diffusion schedule. 

Diffusion models were first introduced by J. Sohl-Dickstein et al. (Deep Unsupervised Learning using Nonequilibrium Thermodynamics) as a flexible and tractable machine learning model. They describe the forward diffusion process of systematically destroying the structure in a data distribution and the learning of a backwards process to restore the structure. The method uses a Markov chain to convert $ x_t $ into $ x_{t-1} $. Starting with $ x_T $, a sample of Gaussian noise, a generative Markov chain converts this into $ x_0 $, which is a sample from the target data distribution. Because the model only estimates small pertubations of noise, $ x_t $ given $ x_{t-1} $ , rather than an entire transformation from $ x_0 $ to $ x_T $, it is tractable to train. 
% does the backwards process definitely start at random noise?

% Generative Modeling by Estimating Gradients of the Data Distribution
This was then built on by Y. Song and S. Ermon in (Generative Modeling ny Estmating Gradients of the Data Distribution) by using Langevin dynamics 
% wtf are these?
% I think it allows the model to learn faster I think
% more scalable than the work by J. Sohl-Dickstein

The DDPM a diffusion model based on a UNet, introduced by J. Ho et al. (Denoising Diffusion Probabilistic Models) is capable of producing high quality images, they show that it is capable of achieving state of the art FID scores across the CIFAR10 dataset. Dhariwal and Nichol (Diffusion Models Beat GANs on Image Synthesis) show tweaks that allow for diffusion models to achieve state of the art FID scores across the ImageNet dataset and when used in combination with upsampling diffusion further improve FID scores. They do this by using improvements proposed by Song et al. [Denoising diffusion implicit models], Nichol and Dhariwal [Improved denoising diffusion probabilistic models], Song et al. [Score-based generative modeling through stochastic differential equations], [Large scale gan training for high fidelity natural image synthesis] [A style based generator architecture for generative adversarial networks]. Through these improvements they are able to improve image quality while reducing the number of noise steps from 1000 to (in some cases) 50. Through the decrease in noise steps they are able to reduce the amount of time that it takes to generate an image.  

% Elucidating the Design Space of Diffusion-Based Generative Models

% On the importance of noise scheduling for diffusion models
Chen discusses in [On the importance of noise scheduling] how changing the resolution of an image has an impact on the noise scheduling required, he finds that at a smaller resolution the optimal schedule may cause under training for higher resolution images. Multiple strategies are proposed to adjust noise scheduling. Firstly, changing the noise schedule functions to those based on cosine or sigmoid, with temparature scaling. Secondly, reducing the input scaling factor from 1 increases the noise levels which destroys more information at the same noise level. They then combine these into a compound noise scheduling strategy.


\subsection{Alternatives}
% A look at resnets

\newpage{}

\chapter{Methodology}

% I dont think this needs to be long, ~2 pages
% This should be from a theoretical perspective
% No need for specific values, just describe what I plan on doing
% Use general ranges of the params

% Motivation?

\section{Requirements Capture}

The VISCHEMA dataset[Vischema paper] contains image to vms mappings. I aim to use a deep learning model to learn a mapping of these images to their corresponding labels. Our model should learn a general understanding of the mapping such that when it is provided with an image that matches our distribution, it can accurately create a VMS label for it. Our model will need to learn to create accurate mappings and we can test that through a loss function, such as L1, and through qualitative analysis.

\section{Motivation}

As standard backpropagation, GANs, and diffusion all produce images in different ways I think it would be interesting to compare how the 3 of them perform when asked the same task. I will run three experiments: I will train a UNet, a GAN, and a diffusion model to produce a VMS label given an image from the dataset. I hope to be able to find the strengths and weaknesses of each of these systems in this application.

For each system I will experiment with network parameters and training hyperparameters to fine tune models that produce the best output. I will experiment with training the models to produce the sum of the images and the VMS labels, and with training the models to produce just the VMS label. In the former the VMS label can be calculated from the output. 


\section{Experiment 1}

\subsection{Implementation}
\subsection{Model Architectures}
\subsection{Training Strategy}

\section{Experiment 2}

I will train a conditional generative adversarial network to generate images of VMS maps given images from the VISCHEMA dataset. The Generator will implemented using a UNet and the Discriminator will be a fully convolutional PatchGAN classifier as described by Isola et al. in [Pix2Pix paper], producing a 4x4 array of boolean values, describing whether a region of the input image is from the datset, or if it is generated.

\textbf{Generator}: This network takes as input a 64x64x3 tensor of floating point values in the range [-1,1]
% double check this input range
It outputs a 64x64x3 tensor of floating point values in the range [-1,1].
% double check this output range
This model can be described as $ L_{f} = G(I) $

\textbf{Discriminator}: This network takes as input a 64x64x6 tensor of floating point values in the range [-1,1]. The first 3 channels are an image and the second 3 channels are its corresponding label, either real or fake.
% double check this
It outputs a 4x4x1 tensor of boolean values. Each value in this output represents a 16x16x3 region of the input.
This model can be described as $ \{0|1\}^{16} = D(L) $

Training within a GAN is typically unstable [papers showing instability in GANs], if one performs much better than the other it can become impossible for the other to improve, therefore I will experiment with different values for the number of layers and channels in each network to try and reduce this instability. I will also automate a system to vary the normalisation method, optimisation function, learning rate, and batch size and indicate the best training environment.

It is not be feesible to test every combination of these variables. Instead I employ a training strategy to test every variation of a single parameter/hyperparameter, pick the best, and do this looping through every parameter. I will start with choices that I estimate to be good, as this should only improve on them. This will bring the size our search space down by an order of magnitude.  

The loss for each network is computed as described in [pix2pix paper]. The loss is computed across an entire batch of images and then the weights are adjusted with backpropagation.

\textbf{Generator Loss}:
The generator loss describes how well it can trick the discriminator and how closely its output matches the real label for the given image.
$ L = MSE( D(L_{f}, I), 1 ) + L1(L_{f}, L_{r}) $

\textbf{Discriminator Loss}: When training the Discriminator the loss is calculated as the mean value of how close our prediction was to the correct answer across a fake and real label.
$ L = 0.5 \times  ( MSE( D( L_{f}, I ), 0) + MSE( D( L_{r}, I ), 1) ) $

Because these two loss values are relative to the performance of eachother they can't be used to see if our generator has converged on a solution. Therefore it is necessary at each epoch to also compute the L1 loss of the fake labels against the real labels across the validation dataset, as this value is independent of the discriminator. This will inform me of whether the network is overfitting, underfitting, or training well. 

\textbf{Training Strategy}: Please see algorithm \ref{ALG:GAN} for the training loop.

\begin{algorithm}
\caption{GAN Training Strategy}\label{ALG:GAN}
\begin{algorithmic}[1]
\For{$I, L_{r} \texttt{in training dataloader}$}
\State
\State $L_{f} = G(I)$
\State
\State $pred_fake = D(L_{f}, I) $
\State $loss_G = MSE( pred_fake, 1 ) + L1(L_{f}, L_{r}) $
\State $\texttt{Update weights of G with backpropagation}$
\State
\State $pred_real = D(L_{r}, I)$
\State $loss_D = 0.5 \times  ( MSE( pred_fake, 0) + MSE( pred_real, 1) ) $
\State $\texttt{Update weights of D with backpropagation}$
\State
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experiment 3}

I will train a conditional generative adversarial network to generate images of VMS maps given images from the VISCHEMA dataset. The Generator will implemented using a UNet and the Discriminator will be a fully convolutional PatchGAN classifier as described by Isola et al. in [Pix2Pix paper], producing a 4x4 array of boolean values, describing whether a region of the input image is from the datset, or if it is generated.

\textbf{Diffusion Model}: This network takes as input a 64x64x3 tensor of floating point values in the range [-1,1]
% double check this input range
It outputs a 64x64x3 tensor of floating point values in the range [-1,1].
% double check this output range
This model can be described as $ L_{f} = G(I) $

Training within a GAN is typically unstable [papers showing instability in GANs], if one performs much better than the other it can become impossible for the other to improve, therefore I will experiment with different values for the number of layers and channels in each network to try and reduce this instability. I will also automate a system to vary the normalisation method, optimisation function, learning rate, and batch size and indicate the best training environment.

It is not be feesible to test every combination of these variables. Instead I employ a training strategy to test every variation of a single parameter/hyperparameter, pick the best, and do this looping through every parameter. I will start with choices that I estimate to be good, as this should only improve on them. This will bring the size our search space down by an order of magnitude.  

The loss for each network is computed as described in [pix2pix paper]. The loss is computed across an entire batch of images and then the weights are adjusted with backpropagation.

\textbf{Model Loss}:
The model loss describes how well it can trick the discriminator and how closely its output matches the real label for the given image.
$ L = MSE( D(L_{f}, I), 1 ) + L1(L_{f}, L_{r}) $

% This but for diffusion
Because these two loss values are relative to the performance of eachother they can't be used to see if our generator has converged on a solution. Therefore it is necessary at each epoch to also compute the L1 loss of the fake labels against the real labels across the validation dataset, as this value is independent of the discriminator. This will inform me of whether the network is overfitting, underfitting, or training well. 

\textbf{Training Strategy}: Please see algorithm \ref{ALG:Diffusion} for the training loop.

\begin{algorithm}
\caption{GAN Training Strategy}\label{ALG:Diffusion}
\begin{algorithmic}[1]
\For{$I, L_{r} \texttt{in training dataloader}$}
\State
\State $L_{f} = G(I)$
\State
\State $pred_fake = D(L_{f}, I) $
\State $loss_G = MSE( pred_fake, 1 ) + L1(L_{f}, L_{r}) $
\State $\texttt{Update weights of G with backpropagation}$
\State
\State $pred_real = D(L_{r}, I)$
\State $loss_D = 0.5 \times  ( MSE( pred_fake, 0) + MSE( pred_real, 1) ) $
\State $\texttt{Update weights of D with backpropagation}$
\State
\EndFor
\end{algorithmic}
\end{algorithm}


\subsection{Implementation}
\subsection{Model Architectures}
\subsection{Training Strategy}

\section{Results Analysis/Testing}

After performing all 3 experiments I will compare the results across the validation dataset qualitatively and using the L1 loss. The L1 loss will tell me how statistically close the outputs are but through qualitative analysis I can see if the outputs have cheated. 
% need to explain cheating better but basically the large blobs
I can also compare the L1 loss across the training and validation sets to see if any models have generalised well, if the loss across the training dataset is much smaller/larger than the validation dataset then it will imply that the model has become overfit/underfit respectively. Ideally they should be similar. 

\newpage{}

\chapter{Experimental Results}

% Approx 10-15 pages

% add plots and tables of results and explain them
% important facts in text
% compare loss functions
% explain dataset limitations
%    solved with gan
%    in experiments?

\section{Experimental Environment}

I have implement the experiments in Python using the PyTorch machine learning framework, however the methodology that I describe should produce the same results in any programming language or framework.

This model was trained on a computer using an RTX 3070 with 8GB of VRAM and an AMD Ryzen 3600 with 32GB of system RAM.
Testing the hyperparameter options for experiment 1 took approximately X days and I trained the final model over the course of Y hours
% FILL IN X AND Y
Testing the hyperparameter options for experiment 2 took approximately 3 days and I trained my final model over the course of 5 hours.
% Need to double check that  
Testing the hyperparameter options for experiment 1 took approximately X days and I trained the final model over the course of Y hours
% FILL IN X AND Y


\section{Experiment 1}

\section{Experiment 2}

In order to find which parameter and hyperparameter combinations would perform the best I automated a training process which would vary the following:
\begin{enumerate}
\item The normalisation layer for both the generator and the discriminator.
\item The channel layouts for the generator and the discriminator.
\item The optimiser used for the generator and the discriminator.
\item The learning rates used for each optimiser.
\item If the Adam optimiser was found to be ideal for either the generator or discriminator, then to use different beta options.
\end{enumerate}

\textbf{I explored the following options}:

\textbf{Normalisation layers}: 
\begin{itemize}
\item Batch Normalisation
\item Instance Norm
\end{itemize}
\textbf{Generator channel layouts}:
\begin{itemize}
\item encoder:(3, 64, 128, 256, 512, 1024), decoder:(1024, 512, 256, 128, 64),
\item encoder:(3, 32, 64, 128, 256, 512), decoder:(512, 256, 128, 64, 32),
\item encoder:(3, 50, 100, 200, 400, 800), decoder:(800, 400, 200, 100, 50),
\item encoder:(3, 100, 200, 400, 800, 1600), decoder:(1600, 800, 400, 200, 100)
\end{itemize}
\textbf{Discriminator channel layouts}:
\begin{itemize}
\item (6, 64, 128, 256, 512, 1024),
\item (6, 32, 64 , 128, 256, 512),
\item (6, 50, 100, 200, 400, 800),
\item (6, 100, 200, 400, 800, 1600)
\end{itemize}
\textbf{Optimisers}:
\begin{itemize}
\item SGD, using the following learning rates (0.005, 0.01, 0.02)
\item Adam, using the following learning rates (0.0005, 0.001, 0.002), and using the following betas ((0.9, 0.999), (0, 0.999), (0.5, 0.999))
\item Adadelta, using the following learning rates (0.5, 1, 2)
\end{itemize}
Exploring this search space exhaustively is unfortunately not feesible, there are over 5000 different combinations possible, and if I tested each combination once for 100 epochs then it would take approximately 300 days to test.
% how long does it take to train an epoch?
Instead I will have to explore a subset of this search space. I estimated some good default parameter and hyperparameter values, by varying these values I can lower the scope of the search space to 
% maybe I should get an exact value here instead of 100
approximately 100 combinations, which took \(\sim\)3 days to test. Unfortunately this did mean that every combination hasn't been tested, however it should give us a good approximation of the best combination.

\emph{Note: I didn't need to specify default values for the normalisation layer as these were the first variables I tested.}

% is combination the best word?
\textbf{Default Generator Parameters and Hyperparameters}: 
\begin{itemize}
\item Encoder Channel Layout: (3,64,128,256,512,1024)
\item Decoder Channel Layout: (1024,512,256,128,64)
\item Optimiser: Adam, betas = (0.9, 0.999)
\item Learning Rate: 0.001
\end{itemize}
\textbf{Default Discriminator Parameters and Hyperparameters}:
\begin{itemize}
\item Channel Layout: (6,64,128,256,512,1024)
\item Optimiser: Adam, betas = (0.9, 0.999)
\item Learning Rate: 0.001
\end{itemize}
I then iterated over each different parameter and hyperparameter and varied each one sequentially. I tested each combination for 100 epochs and used the L1 loss across the validation dataset as the score, if any parameter options performed better than the default then they would be used going forward. This meant that I only had to iterate \(\sim\)100 combinations. This took 3 days to test, and at the end I found that the following combination was the best:

\textbf{Best Generator Parameters and Hyperparameters}:
\begin{itemize}
\item Normalisation Layer: Batch Normalisation 
\item Encoder Channel Layout: (3,32,64,128,256,512)
\item Decoder Channel Layout: (512, 256, 128, 64,32)
\item Optimiser: SGD
\item Learning Rate: 0.01
\end{itemize}
\textbf{Best Discriminator Parameters and Hyperparameters}:
\begin{itemize}
\item Normalisation Layer: Batch Normalisation 
\item Channel Layout: (6,100,200,400,800,1600)
\item Optimiser: Adam, betas = (0.9, 0.999)
\item Learning Rate: 0.001
\end{itemize}

With this combination we achieved a loss of \(\sim\)1.02 across the validation dataset. I found other good results using similar combinations. Using the Adam optimiser for both the generator and discriminator achieved a loss of \(\sim\)1.04 across the validation dataset. Using the Adatelta optimiser for the generator and the Adam optimiser for the discriminator achieved a loss of \(\sim\)1.03 across the validation dataset.

%Insert graph of the best combinations results here


\section{Experiment 3}

\section{Comparisons}

\textbf{Varying Normalisation}

\textbf{Varying Optimiser}

\textbf{Varying Loss Function}

\textbf{PatchGAN vs Boolean Discriminator}

\textbf{Benefits of Residual Connections}

\section{Results}

\section{Evaluation Metrics}

\section{Analysis}


\newpage{}

\chapter{Conclusion}

% Approx 1-2 pages
% how it went
% future work better generator better result
% applications

\newpage{}

\printbibliography

\end{document}