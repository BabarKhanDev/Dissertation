{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import interpolate\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to upload dataset to google colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import zipfile\n",
    "\n",
    "    path_to_zip_file = 'drive/MyDrive/data.zip'\n",
    "    directory_to_extract_to = 'VISCHEMA_PLUS/'\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "    !cp \"drive/MyDrive/viscplus_train.csv\" \"VISCHEMA_PLUS/\"\n",
    "    !cp \"drive/MyDrive/viscplus_val.csv\" \"VISCHEMA_PLUS/\"\n",
    "except:\n",
    "    print(\"not on google colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66699de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "class VISCHEMA_PLUS(Dataset):\n",
    "    def __init__(self, dataset_dir = '../VISCHEMA_PLUS/', image_dir = 'images/', label_dir = 'vms/', train = True, transform = None):\n",
    "        \n",
    "        if train:\n",
    "            train_csv = pd.read_csv(f\"{dataset_dir}viscplus_train.csv\", header = None)\n",
    "            all_images = train_csv[0].values.tolist()\n",
    "        else:\n",
    "            val_csv = pd.read_csv(f\"{dataset_dir}viscplus_val.csv\" , header = None)\n",
    "            all_images = val_csv[0].values.tolist()\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.dataset_dir = dataset_dir\n",
    "        \n",
    "        self.all_images = natsorted(all_images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):    \n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        \n",
    "        image = Image.open(f\"{self.dataset_dir}{self.image_dir}{self.all_images[idx]}\").convert(\"RGB\")\n",
    "        image = convert_tensor(image)\n",
    "\n",
    "        label = Image.open(f\"{self.dataset_dir}{self.label_dir}{self.all_images[idx]}\").convert(\"RGB\")\n",
    "        label = convert_tensor(label)\n",
    "        \n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = VISCHEMA_PLUS(transform = image_transforms, train=True)\n",
    "val_dataset   = VISCHEMA_PLUS(transform = image_transforms, train=False)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(dataset = val_dataset,   batch_size = batch_size, shuffle=True)\n",
    "\n",
    "print(f'{len(train_dataset)} Items in Train dataset')\n",
    "print(f'{len(val_dataset)}  Items in Validation dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4d189",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test that the images and labels look good\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "figure = plt.figure(figsize=(15,15))\n",
    "cols, rows = 1, 4\n",
    "for i in range(cols * rows):\n",
    "    figure.add_subplot(rows,cols*2, 2*i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((images[i,:].squeeze().permute(1, 2, 0) +1 )/2 )\n",
    "    figure.add_subplot(rows,cols*2, 2*i+2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((labels[i,:].squeeze().permute(1, 2, 0) +1 )/2 )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde72fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET Model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm = True, norm_func = nn.InstanceNorm2d):\n",
    "        super().__init__()\n",
    "        if norm:\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "                nn.ReLU(),\n",
    "                norm_func(out_channels),\n",
    "                nn.Conv2d(out_channels,out_channels,3,padding=1),\n",
    "                nn.ReLU(),\n",
    "                norm_func(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels,out_channels,3,padding=1),\n",
    "                nn.ReLU(),\n",
    "                norm_func(out_channels),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=(3,64,128,256,512), norm_func = nn.InstanceNorm2d):\n",
    "        super().__init__()\n",
    "        self.encoding_blocks = nn.ModuleList(\n",
    "            [Block(channels[i], channels[i+1], norm = (i != 0), norm_func = norm_func) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for block in self.encoding_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "                 \n",
    "        return features\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=(512,256,128,64), norm_func = nn.InstanceNorm2d):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(self.channels[i], self.channels[i+1],2,2) for i in range(len(channels)-1)])\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [Block(channels[i], channels[i+1], norm_func = norm_func) for i in range(len(channels)-1)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.channels)-1):\n",
    "            x = self.upconvs[i](x)\n",
    "            features = self.crop(encoder_features[i], x)\n",
    "            x = torch.cat([x, features], dim=1)\n",
    "            x = self.decoder_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, features, x):\n",
    "        _, _, height, width = x.shape\n",
    "        features = transforms.CenterCrop([height, width])(features)\n",
    "        return features\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encode_channels=(3,64,128,256,512,1024), \n",
    "                 decode_channels=(1024,512,256,128,64), \n",
    "                 num_class=3, \n",
    "                 retain_dim=True, \n",
    "                 output_size=(572,572),\n",
    "                 norm_func = nn.InstanceNorm2d):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(encode_channels, norm_func = norm_func)\n",
    "        self.decoder = Decoder(decode_channels, norm_func = norm_func)\n",
    "        \n",
    "        self.head = nn.Conv2d(decode_channels[-1], num_class, 1)\n",
    "        \n",
    "        \"\"\"# Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoding_features = self.encoder(x)\n",
    "        decoding_features = self.decoder(encoding_features[::-1][0], encoding_features[::-1][1:])\n",
    "        output = self.head(decoding_features)\n",
    "        output = nn.Tanh()(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = Generator()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_params} Parameters in UNet Generator')\n",
    "\n",
    "images, _ = next(iter(train_loader))\n",
    "output = model(images)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a9a32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# default output test\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "figure = plt.figure(figsize=(15,15))\n",
    "output = model(images)\n",
    "\n",
    "ideal = np.add(images, labels) \n",
    "\n",
    "cols, rows = 2, 4\n",
    "for i in range(rows):\n",
    "    figure.add_subplot(rows,cols, cols*i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(((ideal[i,:]*0.5).cpu().detach().squeeze().permute(1, 2, 0) +1 )/ 2 )\n",
    "    figure.add_subplot(rows,cols, cols*i+2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((output[i,:].cpu().detach().squeeze().permute(1, 2, 0) +1 )/ 2 )  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our training environment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "images, _ = next(iter(train_loader))\n",
    "_ , _ , height, width = images.shape\n",
    "model = UNet(retain_dim=True, output_size=(height,width))\n",
    "model = model.to(device)\n",
    "\n",
    "loss_func = nn.L1Loss(reduction = 'mean')\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 40\n",
    "\n",
    "# These variables will store the data for analysis\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "\n",
    "os.makedirs(os.path.dirname(\"checkpoints\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d963c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "output = model(images)\n",
    "loss = loss_func(output,labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a050268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training')\n",
    "\n",
    "for epoch in range(1,num_epochs):\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'checkpoints/{epoch}.pkl')\n",
    "        \n",
    "    \n",
    "    # Go into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Train the model and evaluate on the training set\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "    \n",
    "        # Move images to device and create an image prediction\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "    \n",
    "        #Evaluate the loss of our model and take a step\n",
    "        loss = loss_func(output,labels)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        total_train_loss += loss*images.shape[0]\n",
    "        \n",
    "        del images, labels, output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    total_train_loss /= len(train_dataset)\n",
    "    training_losses.append(total_train_loss.to(\"cpu\"))\n",
    "\n",
    "    # Evaluate the model on the val set\n",
    "    # Reset counters and switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            loss = loss_func(output,labels)\n",
    "            total_val_loss += loss*images.shape[0]\n",
    "            \n",
    "            del images, labels, output\n",
    "            \n",
    "    total_val_loss /= len(val_dataset)        \n",
    "    val_losses.append(total_val_loss.item()) \n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}],Train Loss: {total_train_loss}, Val Loss: {total_val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c453b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Training curve\")\n",
    "\n",
    "print(training_losses)\n",
    "print(val_losses)\n",
    "plt.plot(range(len(training_losses)),training_losses,'r')\n",
    "plt.plot(range(len(val_losses)),val_losses,'g')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e59b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoints/final_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(val_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "figure = plt.figure(figsize=(15,15))\n",
    "output = model(images)\n",
    "cols, rows = 1, 4\n",
    "for i in range(cols * rows):\n",
    "    figure.add_subplot(rows,cols*3, 3*i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((images[i,:].cpu().detach().squeeze().permute(1, 2, 0) +1 )/2 )\n",
    "    figure.add_subplot(rows,cols*3, 3*i+2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((labels[i,:].cpu().detach().squeeze().permute(1, 2, 0) +1 )/2 )\n",
    "    figure.add_subplot(rows,cols*3, 3*i+3)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((output[i,:].cpu().detach().squeeze().permute(1, 2, 0) +1 )/2 )\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d3171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
